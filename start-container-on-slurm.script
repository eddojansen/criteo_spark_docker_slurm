#!/bin/bash
#SBATCH --partition=batch
#SBATCH --distribution=arbitrary
#SBATCH --output=outfile-%J
##SBATCH --time=600:00
#SBATCH --nodes=1
#SBATCH --nodelist=ejansen-worker-1
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=0
#SBATCH --mem-per-cpu=3072
#SBATCH --ntasks-per-node=1
#SBATCH hetjob
#SBATCH --output=outfile-%J
#SBATCH --nodes=4
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=2
#SBATCH --mem-per-cpu=3072
#SBATCH --ntasks-per-node=1

set -eux

## Provide mountpoint for shared filesystem for config
export MOUNT=/data

## Criteo test settings
export STARTDAY=0
export ENDDAY=23
export FREQUENCY_LIMIT=15

## Enable or disable GPU with "true" or "false"
export ENABLE_GPU="true"

## Set threads per GPU
export CONCURRENTGPU=1

## Set shuffle.partitions, depends on the dataset size: 1T=1048576M / 200 = 5242
export SHUFFLE_PARTITIONS=5242

## Set Spark SQL partition size
export MAXPARTITIONBYTES=128

## Configure driver memory
export DRIVER_MEMORY=20480

## When using file provide the mountpoint for the criteo dataset and adjust the INPUTH_PATH for file access
## when not used leave empty
##export CRITEO_DATA=/data/days
export CRITEO_DATA=""
## Adjust the INPUTH_PATH for file access
##export INPUT_PATH="file:///opt/criteo/days"

## When using S3 for the data set, change the below values accordingly
export INPUT_PATH="gs://ec-benchmark-data/criteo"
export OUTPUT_PATH="file:///opt/results"
export S3_ENDPOINT="https://storage.googleapis.com"
export S3A_CREDS_USR=""
export S3A_CREDS_PSW=""

##export WAREHOUSE_PATH="file:///opt/criteo/warehouse"
#export WAREHOUSE_PATH="gs://ec-benchmark-data/criteo"
export WAREHOUSE_PATH="file:///opt/criteo"

## No need to change these values as they are
## final mapped destinations for within the container
## and/or dynamic values for slurm
export CRITEO_HOME=/opt/criteo
export SCRIPT=${CRITEO_HOME}/spark_data_utils.py
export TOTAL_CORES=$(( ${SLURM_CPUS_PER_TASK_HET_GROUP_1} * ${SLURM_JOB_NUM_NODES_HET_GROUP_1} ))

if [ ${ENABLE_GPU} = "false" ]
  then 
       export GPU_PER_NODE=0
       export NUM_EXECUTORS=${SLURM_JOB_NUM_NODES_HET_GROUP_1}
       export RESOURCE_GPU_AMT=0
       export WORKER_OPTS=""
       export CONCURRENTGPU=1
       export NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))

  else 
       export GPU_PER_NODE=${SLURM_GPUS_PER_NODE_HET_GROUP_1}
       export NUM_EXECUTORS=$(( ${SLURM_GPUS_PER_NODE_HET_GROUP_1} * ${SLURM_JOB_NUM_NODES_HET_GROUP_1} ))
       export NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
       export WORKER_OPTS="-Dspark.worker.resource.gpu.amount=${GPU_PER_NODE} -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
fi

export PATH=${PATH}:${SPARK_HOME}/bin:${SPARK_HOME}/sbin
export MASTER=`hostname`
export EXECUTOR_MEMORY=$(( ${TOTAL_CORES} * ${SLURM_MEM_PER_CPU_HET_GROUP_1} / ${NUM_EXECUTORS} ))
export SPARK_WORKER_CORES=`nproc`
export SPARK_WORKER_MEMORY=$(( ${TOTAL_CORES} * ${SLURM_MEM_PER_CPU_HET_GROUP_1} / ${SLURM_JOB_NUM_NODES_HET_GROUP_1} ))
export PINNED_POOL_SIZE=$(( ${SPARK_WORKER_MEMORY} / 2 / $GPU_PER_NODE ))

sudo mkdir -p ${MOUNT}/conf
sudo chown -R $(id -u):$(id -g) ${MOUNT}/conf
sudo cp wait-worker.sh ${MOUNT}/conf/wait-worker.sh
sudo chmod +x ${MOUNT}/conf/wait-worker.sh
sudo cp kill* ${MOUNT}/conf
sudo chmod +x ${MOUNT}/conf/kill*.sh
sudo mkdir -p ${MOUNT}/criteo
sudo chown -R $(id -u):$(id -g) ${MOUNT}/criteo
sudo cp submit_criteo.sh ${MOUNT}/criteo/submit_criteo.sh
sudo chmod +x ${MOUNT}/criteo/submit_criteo.sh
sudo mkdir -p ${MOUNT}/logs
sudo chown -R $(id -u):$(id -g) ${MOUNT}/logs
sudo mkdir -p ${MOUNT}/results/models
sudo chown -R $(id -u):$(id -g) ${MOUNT}/results
sudo mkdir -p ${MOUNT}/history
sudo chown -R $(id -u):$(id -g) ${MOUNT}/history

srun --het-group=0 -w `hostname` ${MOUNT}/conf/kill-master.sh || true &&
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" ${MOUNT}/conf/kill-worker.sh || true &&
srun --het-group=0 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_0}" bash -c "echo -n 'Clearing cache on ' && sync && sudo /sbin/sysctl vm.drop_caches=3"
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" bash -c "echo -n 'Clearing cache on ' && sync && sudo /sbin/sysctl vm.drop_caches=3"

scontrol show hostname $SLURM_JOB_NODELIST_HET_GROUP_1 > ${MOUNT}/conf/slaves

conf=${MOUNT}/conf/spark-defaults.conf
echo "spark.default.parallelism" $(( ${NUM_EXECUTORS} )) > $conf
echo "spark.submit.deployMode" client >> $conf
echo "spark.master" spark://`hostname`:7077 >> $conf
echo "spark.executor.cores" ${NUM_EXECUTOR_CORES} >> $conf
echo "spark.executor.memory" ${EXECUTOR_MEMORY}M >> $conf
echo "spark.eventLog.enabled" true >> $conf
echo "spark.eventLog.dir" file:/opt/spark/history >> $conf
echo "spark.history.fs.logDirector" file:/opt/spark/history >> $conf

## Enable when existing image needs to be deleted
##srun --het-group=1 -n 1 -N 1 -w `hostname` docker rmi  gcr.io/data-science-enterprise/spark-slurm-master:3.0.1 || true 

srun --het-group=0 -n 1 -N 1 -w `hostname` docker run -dit \
-e MASTER="${MASTER}" \
-e CRITEO_HOME="${CRITEO_HOME}" \
-e SCRIPT="${SCRIPT}" \
-e FREQUENCY_LIMIT="${FREQUENCY_LIMIT}" \
-e STARTDAY="${STARTDAY}" \
-e ENDDAY="${ENDDAY}" \
-e ENABLE_GPU="${ENABLE_GPU}" \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-e CONCURRENTGPU="${CONCURRENTGPU}" \
-e TOTAL_CORES="${TOTAL_CORES}" \
-e NUM_EXECUTORS="${NUM_EXECUTORS}" \
-e NUM_EXECUTOR_CORES="${NUM_EXECUTOR_CORES}" \
-e EXECUTOR_MEMORY="${EXECUTOR_MEMORY}M" \
-e PINNED_POOL_SIZE="${PINNED_POOL_SIZE}M" \
-e DRIVER_MEMORY="${DRIVER_MEMORY}M" \
-e SHUFFLE_PARTITIONS="${SHUFFLE_PARTITIONS}" \
-e MAXPARTITIONBYTES="${MAXPARTITIONBYTES}M" \
-e S3A_CREDS_USR="${S3A_CREDS_USR}" \
-e S3A_CREDS_PSW="${S3A_CREDS_PSW}" \
-e S3_ENDPOINT="${S3_ENDPOINT}" \
-e OUTPUT_PATH="${OUTPUT_PATH}" \
-e INPUT_PATH="${INPUT_PATH}" \
-e PATH="$PATH" \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}/history:/opt/spark/history \
-v ${MOUNT}/criteo/submit_criteo.sh:/opt/criteo/submit_criteo.sh \
-v ${MOUNT}/results:/opt/results \
-v ${CRITEO_DATA}:/opt/criteo/days \
-v /tmp:/tmp \
--network host \
--name master \
--rm \
 gcr.io/data-science-enterprise/spark-slurm-master:3.0.1

## Enable when existing image needs to be deleted
##srun --het-group=0 --ntasks=${SLURM_JOB_NUM_NODES_HET_GROUP_0} --ntasks-per-node=1 docker rmi gcr.io/data-science-enterprise/spark-slurm-worker-rapids-cuda:3.0.1 || true

srun --het-group=1 --ntasks=${SLURM_JOB_NUM_NODES_HET_GROUP_1} --ntasks-per-node=1 docker run -dit \
-e MASTER=${MASTER} \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_MEMORY="${SPARK_WORKER_MEMORY}M" \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}/history:/opt/spark/history \
-v ${MOUNT}/results:/opt/results \
-v ${CRITEO_DATA}:/opt/criteo/days \
-v /tmp:/tmp \
--network host \
--name worker \
--rm \
gcr.io/data-science-enterprise/spark-slurm-worker-rapids-cuda:3.0.1

srun --het-group=0 -n 1 -N 1 -w `hostname` bash -c $MOUNT/conf/wait-worker.sh

echo "All workers registered!"

srun --het-group=0 -n 1 -N 1 -w `hostname` docker exec -i master bash -c /opt/criteo/submit_criteo.sh

#
sleep infinity
#

echo "test complete, check ${MOUNT}/results and ${MOUNT}/history" 
srun --het-group=0 -n 1 -N 1 --gpus=0 -w `hostname`  ${MOUNT}/conf/kill-master.sh || true
srun --het-group=1 --ntasks="${SLURM_JOB_NUM_NODES_HET_GROUP_1}" ${MOUNT}/conf/kill-worker.sh || true
echo "Cleanup complete, bye bye.."
